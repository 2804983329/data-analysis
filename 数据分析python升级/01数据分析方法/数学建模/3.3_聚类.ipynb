{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "【课程3.4.2】  PCA主成分分析的python实现方法\n",
    "\n",
    "最广泛无监督算法 + 基础的降维算法\n",
    "通过线性变换将原始数据变换为一组各维度线性无关的表示，用于提取数据的主要特征分量 → 高维数据的降维\n",
    "\n",
    "二维数据降维 / 多维数据降维\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 二维数据降维\n",
    "# 数据创建\n",
    "rng = np.random.RandomState(8)\n",
    "data = np.dot(rng.rand(2,2),rng.randn(2,200)).T\n",
    "df = pd.DataFrame({'X1':data[:,0],\n",
    "                    'X2':data[:,1]})\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "plt.scatter(df['X1'],df['X2'], alpha = 0.8, marker = '.')\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "# 生成图表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 二维数据降维\n",
    "# 构建模型，分析主成分\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# 加载主成分分析模块PCA\n",
    "\n",
    "pca = PCA(n_components = 1)  # n_components = 1 → 降为1维\n",
    "pca.fit(df)  # 构建模型\n",
    "# sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)\n",
    "# n_components:  PCA算法中所要保留的主成分个数n，也即保留下来的特征个数n\n",
    "# copy: True或者False，默认为True → 表示是否在运行算法时，将原始训练数据复制一份\n",
    "# fit(X,y=None) → 调用fit方法的对象本身。比如pca.fit(X)，表示用X对pca这个对象进行训练\n",
    "\n",
    "print(pca.explained_variance_)  # 输出特征值\n",
    "print(pca.components_)  # 输出特征向量\n",
    "print(pca.n_components_)  # 输出成分的个数\n",
    "print('-----')\n",
    "# components_：返回具有最大方差的成分。\n",
    "# explained_variance_ratio_：返回 所保留的n个成分各自的方差百分比。\n",
    "# n_components_：返回所保留的成分个数n。\n",
    "\n",
    "# 这里是shape(200,2)降为shape(200,1)，只有1个特征值，对应2个特征向量\n",
    "# 降维后主成分 A1 = 0.7788006 * X1 + 0.62727158 * X2\n",
    "\n",
    "x_pca = pca.transform(df)  # 数据转换\n",
    "x_new = pca.inverse_transform(x_pca)  # 将降维后的数据转换成原始数据\n",
    "print('original shape:',df.shape)\n",
    "print('transformed shape:',x_pca.shape)\n",
    "print(x_pca[:5])\n",
    "print('-----')\n",
    "# 主成分分析，生成新的向量x_pca\n",
    "# fit_transform(X) → 用X来训练PCA模型，同时返回降维后的数据，这里x_pca就是降维后的数据\n",
    "# inverse_transform() → 将降维后的数据转换成原始数据\n",
    "\n",
    "plt.scatter(df['X1'],df['X2'], alpha = 0.8, marker = '.')\n",
    "plt.scatter(x_new[:,0],x_new[:,1], alpha = 0.8, marker = '.',color = 'r')\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "# 生成图表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 多维数据降维\n",
    "# 加载数据\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits .keys())\n",
    "print('数据长度为:%i条' % len(digits['data']))\n",
    "print('数据形状为:%i条',digits.data.shape)\n",
    "print(digits.data[:2])\n",
    "# 导入数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 多维数据降维\n",
    "# 构建模型，分析主成分\n",
    "\n",
    "pca = PCA(n_components = 2)  # 降为2纬\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print('original shape:',digits.data.shape)\n",
    "print('transformed shape:',projected.shape)\n",
    "print(pca.explained_variance_)  # 输出特征值\n",
    "print(pca.components_.shape)  # 输出特征向量形状\n",
    "#print(projected)  # 输出解析后数据\n",
    "# 降维后，得到2个成分，每个成分有64个特征向量\n",
    "\n",
    "plt.scatter(projected[:,0],projected[:,1],\n",
    "           c = digits.target, edgecolor = 'none',alpha = 0.5,\n",
    "           cmap = 'Reds',s = 5)\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.colorbar()\n",
    "# 二维数据制图"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 多维数据降维\n",
    "# 主成分筛选\n",
    "\n",
    "pca = PCA(n_components = 10)  # 降为10纬\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print('original shape:',digits.data.shape)\n",
    "print('transformed shape:',projected.shape)\n",
    "print(pca.explained_variance_)  # 输出特征值\n",
    "print(pca.components_.shape)  # 输出特征向量形状\n",
    "#print(projected)  # 输出解析后数据\n",
    "# 降维后，得到10个成分，每个成分有64个特征向量\n",
    "\n",
    "c_s = pd.DataFrame({'b':pca.explained_variance_,\n",
    "                   'b_sum':pca.explained_variance_.cumsum()/pca.explained_variance_.sum()})\n",
    "print(c_s)\n",
    "# 做贡献率累计求和\n",
    "# 可以看到第7个成分时候，贡献率超过85% → 选取前7个成分作为主成分\n",
    "\n",
    "c_s['b_sum'].plot(style = '--ko', figsize = (10,4))\n",
    "plt.axhline(0.85,hold=None,color='r',linestyle=\"--\",alpha=0.8)\n",
    "plt.text(6,c_s['b_sum'].iloc[6]-0.08,'第7个成分累计贡献率超过85%',color = 'r')\n",
    "plt.grid()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "【课程3.4.3】  K-means聚类的python实现方法\n",
    "\n",
    "最常用的机器学习聚类算法，且为典型的基于距离的聚类算法\n",
    "K均值： 基于原型的、划分的距离技术，它试图发现用户指定个数(K)的簇\n",
    "以欧式距离作为相似度测度\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 创建数据\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "# make_blobs聚类数据生成器\n",
    "\n",
    "x,y_true = make_blobs(n_samples = 300,   # 生成300条数据\n",
    "                     centers = 4,        # 四类数据\n",
    "                     cluster_std = 0.5,  # 方差一致\n",
    "                     random_state = 0)\n",
    "print(x[:5])\n",
    "print(y_true[:5])\n",
    "# n_samples → 待生成的样本的总数。\n",
    "# n_features → 每个样本的特征数。\n",
    "# centers → 类别数\n",
    "# cluster_std → 每个类别的方差，如多类数据不同方差，可设置为[1.0,3.0]（这里针对2类数据）\n",
    "# random_state → 随机数种子\n",
    "# x → 生成数据值， y → 生成数据对应的类别标签\n",
    "\n",
    "plt.scatter(x[:,0],x[:,1],s = 10,alpha = 0.8)\n",
    "plt.grid()\n",
    "# 绘制图表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 构建K均值模型\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# 导入模块\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)  # 这里为4簇\n",
    "kmeans.fit(x)\n",
    "y_kmeans = kmeans.predict(x)\n",
    "centroids = kmeans.cluster_centers_\n",
    "# 构建模型，并预测出样本的类别y_kmeans\n",
    "# kmeans.cluster_centers_：得到不同簇的中心点\n",
    "\n",
    "plt.scatter(x[:,0],x[:,1],c = y_kmeans, cmap = 'Dark2', s= 50,alpha = 0.5,marker='x')\n",
    "plt.scatter(centroids[:,0],centroids[:,1],c = [0,1,2,3], cmap = 'Dark2',s= 70,marker='o')\n",
    "plt.title('K-means 300 points\\n')\n",
    "plt.xlabel('Value1')\n",
    "plt.ylabel('Value2')\n",
    "plt.grid()\n",
    "# 绘制图表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}